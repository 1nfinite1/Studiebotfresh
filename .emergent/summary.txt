<analysis>
The AI engineer's work on the Studiebot application has been highly iterative, addressing a series of build errors, UI crashes, and functionality gaps reported by the user. Initially, the focus was on fixing Preview, Delete, and LLM context integration, which involved patching , , and . Subsequent build failures revealed syntax issues and HTML entity errors in Javascript files. A major challenge was ensuring LLM context grounding, leading to the implementation of PDF/DOCX text extraction using  and , and later an OCR fallback via . The project then pivoted to a large-scale prompt unification, defining detailed LLM behaviors for Learn, Quiz, and Exam modes, including specific JSON contracts for the Exam grading and the restoration of a legacy Exam UI. Most recently, the Hints feature was entirely removed, and the Overhoren mode was rewired to use a dedicated quiz endpoint. The current state is a critical UI crash, resulting in a blank page, which is attributed to an incorrect  component return and a Tailwind CSS typo.
</analysis>
<product_requirements>
The Studiebot application aims to provide Dutch-language learning modes (Learn, Quiz, Exam) for 12-16 year olds, using LLMs with Bloom taxonomy, adaptive learning, and guardrails. Study materials are stored in MongoDB, with fallback for missing data.
Key features and requirements addressed or modified throughout the trajectory:
1.  **Build Stability**: Fix JSX syntax errors, remove stray  statements, and introduce a  flag. Resolve HTML entity issues breaking Vercel builds.
2.  **Preview Functionality**:  must consistently return , with fallbacks to segments text, GridFS sample, or stub.
3.  **Delete Functionality**: Support  and , performing cascade deletes (materials, segments, GridFS). Must return  on success.
4.  **LLM Context Integration**:  must inject  into all LLM routes (, , , , ). If no active material, return . Responses must reference active material.
5.  **Text Extraction & OCR**: Implement PDF/DOCX text extraction (using , ) on upload/ingest to populate . Add an OCR fallback layer (e.g., ) behind feature flags (, , ) for scanned PDFs.
6.  **LLM Prompt Unification**: Standardize system prompts across Learn, Quiz, and Exam modes to be warm, motivating, Dutch-only, with 2-4 emojis, short feedback, and strict JSON for Exam grading.
7.  **Learn Mode ()**: Guide with tiny activation prompts. One small, concrete activation question per turn. Brief, friendly feedback with emojis, then one new follow-up question. No hint overlay.
8.  **Quiz Mode ()**: Quiz on active material. One question per turn, varying types and adapting difficulty. Encouraging feedback with emojis, correct answer if needed, then one new question. Must use dedicated quiz route.
9.  **Exam Mode ()**:
    *   **Generate**: Produce a full practice exam (default 5, allow 10) in one response, numbered questions, mixed types (short-open, MC, explain). No answers. Legacy UI flow (all questions at once, white text boxes).
    *   **Submit/Grade**: Return strict legacy JSON with score,  (question, studentAnswer, status, emoji, explanation, modelAnswer). Emoji mapping: ‚úÖ, ‚ùå, ü§î. UI renders a white Toetsrapport for results only. Stateless grading (request includes  text).
10. **UI/UX Refinements**: Hints removed entirely. Glossary terms appear bold deep purple, no asterisks, with popover definitions. Ensure consistent UI styling.
11. **Diagnostics**: All LLM routes to include debug headers: , , , , .
</product_requirements>
<key_technical_concepts>
- Next.js 14 (app router, server-only API routes)
- FastAPI (Python backend)
- MongoDB with GridFS (for materials storage)
- LLM integration (OpenAI, potentially Anthropic/Google via Emergent LLM Key)
-  &  (for PDF/DOCX text extraction)
-  (for OCR fallback)
- Consistent JSON API contract with  for all responses
</key_technical_concepts>
<code_architecture>
Next.js app voor frontend, FastAPI backend, MongoDB/GridFS voor materiaalopslag, gescheiden routes per modus (learn, quiz, exam). Prompts gecentraliseerd in infra/llm/prompts
</code_architecture>
<pending_tasks>
- **Hotfix for Blank Page**: The application is currently crashing, resulting in a completely empty page. The immediate task is to fix  by restoring  to return valid JSX and correcting a Tailwind class typo ( to ).
</pending_tasks>
<current_work>
Immediately prior to this summary, the AI engineer had implemented a significant set of changes related to LLM interaction and UI.
1.  **Hints Feature Removal**: The Hints feature was entirely decommissioned from the UI, API, and LLM prompts.  now returns a 410 Gone status.
2.  **Overhoren (Quiz) Rerouting**: The Overhoren mode was rewired to exclusively use the  endpoint for question generation, moving away from a shared learn logic.
3.  **Leren (Learn) Endpoint**: A new  endpoint was introduced for the Leren mode, providing a compact, single-message response without hint overlays.
4.  **Oefentoets (Exam) State**: The Oefentoets endpoints ( and ) and their legacy UI flow (generate all questions, then show white Toetsrapport for results only) remained as previously updated.
5.  **Prompt Updates**: All LLM prompts were fully de-hinted and adjusted to the specified tone, emoji usage, and question/feedback structure.
6.  **Code Cleanup**: Related code paths and test files for the hints feature were removed.

The last action performed was a detailed summary of these changes and verification steps. However, the user then reported a critical error: The page is now fully empty. The AI engineer suspects this is a runtime crash in React, possibly due to  in  returning  and a Tailwind class typo.
</current_work>
<optional_next_step>
Implement a hotfix for  to resolve the blank page error.
</optional_next_step>
